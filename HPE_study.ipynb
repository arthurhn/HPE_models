{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative study of YOLO vs MediaPipe vs MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Load the input image.\n",
    "image_path = 'Yoga poses.v5i.yolov8/test/images/1_123_jpg.rf.38c81030db0d99d8c5a2c090b3028403.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for display with matplotlib\n",
    "\n",
    "# File path to your YOLO txt file\n",
    "file_path = 'Yoga poses.v5i.yolov8/test/labels/1_123_jpg.rf.38c81030db0d99d8c5a2c090b3028403.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 decoding failed, trying ISO-8859-1 encoding...\")\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oks(gt_keypoints, pred_keypoints, bbox_area, indices):\n",
    "    # Object Keypoint Similarity (OKS) is a metric used to evaluate the accuracy of keypoint predictions\n",
    "\n",
    "    sigmas = np.array([0.26, 0.25, 0.25, 0.35, 0.35, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62])\n",
    "    #sigmas = np.array([0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33])\n",
    "\n",
    "    #Filter keypoints\n",
    "    selected_sigmas = []\n",
    "\n",
    "    for j in indices:\n",
    "        selected_sigmas.append(sigmas[j])\n",
    "\n",
    "    #print(\"gt_kpts: \", gt_keypoints)\n",
    "\n",
    "    # Ensure both keypoints lists have exactly 13 keypoints\n",
    "    gt_keypoints = gt_keypoints[:13]\n",
    "    pred_keypoints = pred_keypoints[:13]\n",
    "\n",
    "\n",
    "    y_true = np.array(gt_keypoints).reshape(-1, 2)\n",
    "    y_pred = np.array(pred_keypoints).reshape(-1, 2)\n",
    "\n",
    "    #print(\"ytrue: \", y_true)\n",
    "    #print(\"ypred: \", y_pred)\n",
    "\n",
    "    # Handle cases where there might be fewer keypoints\n",
    "    if y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch between ground truth and predicted keypoints: {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "\n",
    "    # Calculate Euclidean distance between keypoints\n",
    "    d2 = (y_true - y_pred)**2 \n",
    "    # d^2 = (x1 - x2)^2 + (y1 - y2)^2\n",
    "    d2_sum = d2.sum(axis=1)\n",
    "\n",
    "    #print(d2_sum)\n",
    "    \n",
    "\n",
    "    # Adjust sigmas shape if necessary\n",
    "    if sigmas.shape[0] != d2.shape[1]:\n",
    "        sigmas = sigmas[:d2.shape[1]]\n",
    "\n",
    "    # COCO assigns k = 2Ïƒ.\n",
    "    for i in range(len(selected_sigmas)):\n",
    "        selected_sigmas[i] = 2 * selected_sigmas[i]\n",
    "\n",
    "    #print(selected_sigmas)\n",
    "\n",
    "    denom=[]\n",
    "    # Denominator in the exponent term. Shape: [M, 1, #kpts]\n",
    "    for k in range(len(selected_sigmas)):\n",
    "        denom.append( 2 * (selected_sigmas[k]**2) * bbox_area )\n",
    "    \n",
    "    #print( -d2_sum/denom )\n",
    "\n",
    "    # Calculate OKS\n",
    "    oks = np.exp(-d2_sum / denom )\n",
    "\n",
    "    print(oks)\n",
    "\n",
    "    return oks.mean()\n",
    "\n",
    "\n",
    "def calculate_mppe(gt_keypoints, pred_keypoints):\n",
    "    # Mean Per Part Error (MPPE) is the average error between predicted and ground truth parts detected (pairs of keypoints) \n",
    "\n",
    "    # Ensure both keypoints lists have exactly 13 keypoints\n",
    "    gt_keypoints = gt_keypoints[:13]\n",
    "    pred_keypoints = pred_keypoints[:13]\n",
    "\n",
    "    # Handle missing keypoints by replacing them with a placeholder\n",
    "    def handle_missing_keypoints(keypoints):\n",
    "        return [(0, 0) if k == (0, 0) else k for k in keypoints]\n",
    "\n",
    "    gt_keypoints = handle_missing_keypoints(gt_keypoints)\n",
    "    pred_keypoints = handle_missing_keypoints(pred_keypoints)\n",
    "\n",
    "    correct_parts = 0\n",
    "    total_parts = 0\n",
    "    \n",
    "    # COCO limb pairs: COCO has pairs like [5,7] for left upper arm, [11,13] for left upper leg, etc.\n",
    "    # coco_limb_pairs = [(5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "\n",
    "    # Converted limb pairs: pairs are enumerated from 0 to 13, based on kpts array size, check coco_indices for conversion\n",
    "    limb_pairs = [(1, 9), (9, 11), (2, 10), (10, 12), (3, 5), (5, 7), (4, 6), (6, 8)]\n",
    "\n",
    "    error_array = []\n",
    "\n",
    "    for (i, j) in limb_pairs:\n",
    "        #Skip the part if either keypoint in the pair is missing\n",
    "        if gt_keypoints[i] == (0, 0) or gt_keypoints[j] == (0, 0):\n",
    "            continue\n",
    "        \n",
    "        gt_dist = np.linalg.norm(np.array(gt_keypoints[i]) - np.array(gt_keypoints[j]))\n",
    "        pred_dist = np.linalg.norm(np.array(pred_keypoints[i]) - np.array(pred_keypoints[j]))\n",
    "        \n",
    "        # Check if the predicted distance is within a threshold (example: 50% of ground truth distance)\n",
    "        error_array.append( abs(gt_dist - pred_dist) / gt_dist )\n",
    "    \n",
    "    print(error_array)\n",
    "    return np.mean(error_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models and extracting predicted keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "input_size = 192\n",
    "\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores\n",
    "\n",
    "\n",
    "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "\n",
    "#---------------------------------Predicted Keypoints---------------------------------\n",
    "\n",
    "predicted_kpts = []   #normalized\n",
    "\n",
    "selected_indices = [0,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "#Filter the keypoints to only include the ones we want\n",
    "for i in selected_indices:\n",
    "    predicted_kpts.append(keypoints_with_scores[0][0][i])\n",
    "\n",
    "# Convert normalized coordinates to image pixel coordinates\n",
    "pred_kpts = []\n",
    "for kp in predicted_kpts:\n",
    "    x = kp[0] * 640\n",
    "    y = kp[1] * 640\n",
    "    pred_kpts.append([y, x])\n",
    "\n",
    "print(\"Predicted keypoints: \", pred_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    return results, image_rgb\n",
    "\n",
    "results, image_rgb = process_image(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(image_path)\n",
    "\n",
    "# Define the indices for the 13 keypoints we need\n",
    "# Using MediaPipe indices: \n",
    "# 0: Nose, 5: Left Shoulder, 6: Right Shoulder, 11: Left Hip, 12: Right Hip, \n",
    "# 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle, 7: Left Elbow, 8: Right Elbow,\n",
    "# 9: Left Wrist, 10: Right Wrist\n",
    "selected_indices = [0, 5, 6, 11, 12, 13, 14, 15, 16, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "# ----------------------------------Predicted keypoints----------------------------------------------\n",
    "denormalized_kps = []\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    keypoints = r.keypoints.xyn.cpu().numpy()  # Normalized keypoints (x, y, conf)\n",
    "    \n",
    "    for kp in keypoints[0]:\n",
    "            x, y = int(kp[0] * 640), int(kp[1] * 640) # denormalize , if needed\n",
    "            denormalized_kps.append((x,y))\n",
    "\n",
    "pred_kpts = []\n",
    "\n",
    "# Filter for the 13 specific keypoints\n",
    "for i in selected_indices:\n",
    "    pred_kpts.append(denormalized_kps[i])\n",
    "\n",
    "print(\"Predicted keypoints: \", pred_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting ground truth keypoints from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++Ground Truth Keypoints++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "normal_gt_kpts = [float(value) for i, value in enumerate(data.split()) if 0 < float(value) <= 1 ]\n",
    "\n",
    "\n",
    "# Reshape keypoints into pairs of (x, y)\n",
    "normal_gt_kpts = np.array(normal_gt_kpts).reshape(-1, 2)\n",
    "\n",
    "# Denormalize keypoints\n",
    "gt_kpts = np.zeros_like(normal_gt_kpts)\n",
    "gt_kpts[:, 0] = normal_gt_kpts[:, 0] * 640   # Denormalize x by image width\n",
    "gt_kpts[:, 1] = normal_gt_kpts[:, 1] * 640  # Denormalize y by image height\n",
    "\n",
    "\n",
    "#print(gt_kpts[2:])\n",
    "# print(selected_keypoints) visualization and debugging\n",
    "\n",
    "gt_kpts = gt_kpts[2:]   # remove the first two keypoints as they are part of bbox *VERY IMPORTANT*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
