{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative study of YOLO vs MediaPipe vs MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/rtu/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Load the input image.\n",
    "image_path = 'Yoga poses.v5i.yolov8/test/images/1_123_jpg.rf.38c81030db0d99d8c5a2c090b3028403.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for display with matplotlib\n",
    "\n",
    "# File path to your YOLO txt file\n",
    "file_path = 'Yoga poses.v5i.yolov8/test/labels/1_123_jpg.rf.38c81030db0d99d8c5a2c090b3028403.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UTF-8 decoding failed, trying ISO-8859-1 encoding...\")\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_oks(gt_keypoints, pred_keypoints, bbox_area, indices):\n",
    "    # Object Keypoint Similarity (OKS) is a metric used to evaluate the accuracy of keypoint predictions\n",
    "\n",
    "    sigmas = np.array([0.26, 0.25, 0.25, 0.35, 0.35, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62, 0.79, 0.79, 0.72, 0.72, 0.62, 0.62])\n",
    "    #sigmas = np.array([0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33, 0.33])\n",
    "\n",
    "    #Filter keypoints\n",
    "    selected_sigmas = []\n",
    "\n",
    "    for j in indices:\n",
    "        selected_sigmas.append(sigmas[j])\n",
    "\n",
    "    #print(\"gt_kpts: \", gt_keypoints)\n",
    "\n",
    "    # Ensure both keypoints lists have exactly 13 keypoints\n",
    "    gt_keypoints = gt_keypoints[:13]\n",
    "    pred_keypoints = pred_keypoints[:13]\n",
    "\n",
    "\n",
    "    y_true = np.array(gt_keypoints).reshape(-1, 2)\n",
    "    y_pred = np.array(pred_keypoints).reshape(-1, 2)\n",
    "\n",
    "    #print(\"ytrue: \", y_true)\n",
    "    #print(\"ypred: \", y_pred)\n",
    "\n",
    "    # Handle cases where there might be fewer keypoints\n",
    "    if y_true.shape[0] != y_pred.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch between ground truth and predicted keypoints: {y_true.shape} vs {y_pred.shape}\")\n",
    "\n",
    "\n",
    "    # Calculate Euclidean distance between keypoints\n",
    "    d2 = (y_true - y_pred)**2 \n",
    "    # d^2 = (x1 - x2)^2 + (y1 - y2)^2\n",
    "    d2_sum = d2.sum(axis=1)\n",
    "\n",
    "    #print(d2_sum)\n",
    "    \n",
    "\n",
    "    # Adjust sigmas shape if necessary\n",
    "    if sigmas.shape[0] != d2.shape[1]:\n",
    "        sigmas = sigmas[:d2.shape[1]]\n",
    "\n",
    "    # COCO assigns k = 2σ.\n",
    "    for i in range(len(selected_sigmas)):\n",
    "        selected_sigmas[i] = 2 * selected_sigmas[i]\n",
    "\n",
    "    #print(selected_sigmas)\n",
    "\n",
    "    denom=[]\n",
    "    # Denominator in the exponent term. Shape: [M, 1, #kpts]\n",
    "    for k in range(len(selected_sigmas)):\n",
    "        denom.append( 2 * (selected_sigmas[k]**2) * bbox_area )\n",
    "    \n",
    "    #print( -d2_sum/denom )\n",
    "\n",
    "    # Calculate OKS\n",
    "    oks = np.exp(-d2_sum / denom )\n",
    "\n",
    "    print(oks)\n",
    "\n",
    "    return oks.mean()\n",
    "\n",
    "\n",
    "def calculate_mppe(gt_keypoints, pred_keypoints):\n",
    "    # Mean Per Part Error (MPPE) is the average error between predicted and ground truth parts detected (pairs of keypoints) \n",
    "\n",
    "    # Ensure both keypoints lists have exactly 13 keypoints\n",
    "    gt_keypoints = gt_keypoints[:13]\n",
    "    pred_keypoints = pred_keypoints[:13]\n",
    "\n",
    "    # Handle missing keypoints by replacing them with a placeholder\n",
    "    def handle_missing_keypoints(keypoints):\n",
    "        return [(0, 0) if k == (0, 0) else k for k in keypoints]\n",
    "\n",
    "    gt_keypoints = handle_missing_keypoints(gt_keypoints)\n",
    "    pred_keypoints = handle_missing_keypoints(pred_keypoints)\n",
    "\n",
    "    correct_parts = 0\n",
    "    total_parts = 0\n",
    "    \n",
    "    # COCO limb pairs: COCO has pairs like [5,7] for left upper arm, [11,13] for left upper leg, etc.\n",
    "    # coco_limb_pairs = [(5, 7), (7, 9), (6, 8), (8, 10), (11, 13), (13, 15), (12, 14), (14, 16)]\n",
    "\n",
    "\n",
    "    # Converted limb pairs: pairs are enumerated from 0 to 13, based on kpts array size, check coco_indices for conversion\n",
    "    limb_pairs = [(1, 9), (9, 11), (2, 10), (10, 12), (3, 5), (5, 7), (4, 6), (6, 8)]\n",
    "\n",
    "    error_array = []\n",
    "\n",
    "    for (i, j) in limb_pairs:\n",
    "        #Skip the part if either keypoint in the pair is missing\n",
    "        if gt_keypoints[i] == (0, 0) or gt_keypoints[j] == (0, 0):\n",
    "            continue\n",
    "        \n",
    "        gt_dist = np.linalg.norm(np.array(gt_keypoints[i]) - np.array(gt_keypoints[j]))\n",
    "        pred_dist = np.linalg.norm(np.array(pred_keypoints[i]) - np.array(pred_keypoints[j]))\n",
    "        \n",
    "        # Check if the predicted distance is within a threshold (example: 50% of ground truth distance)\n",
    "        error_array.append( abs(gt_dist - pred_dist) / gt_dist )\n",
    "    \n",
    "    print(error_array)\n",
    "    return np.mean(error_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying models and extracting predicted keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728399473.496170    8463 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted keypoints:  [[269.8226737976074, 210.44572830200195], [314.4546318054199, 267.2721862792969], [296.9321060180664, 262.52140045166016], [309.8683738708496, 184.03955459594727], [304.48888778686523, 183.32752227783203], [319.0231704711914, 128.33684921264648], [322.3508834838867, 129.77527618408203], [308.92187118530273, 387.1814727783203], [286.1391258239746, 387.32479095458984], [415.4742431640625, 455.6941223144531], [153.52152824401855, 394.9186325073242], [535.9650421142578, 492.1772003173828], [164.03810501098633, 499.5343017578125]]\n"
     ]
    }
   ],
   "source": [
    "module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "input_size = 192\n",
    "\n",
    "\n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores\n",
    "\n",
    "\n",
    "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
    "input_image = tf.expand_dims(image, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "# Run model inference.\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "\n",
    "#---------------------------------Predicted Keypoints---------------------------------\n",
    "\n",
    "predicted_kpts = []   #normalized\n",
    "\n",
    "selected_indices = [0,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "#Filter the keypoints to only include the ones we want\n",
    "for i in selected_indices:\n",
    "    predicted_kpts.append(keypoints_with_scores[0][0][i])\n",
    "\n",
    "# Convert normalized coordinates to image pixel coordinates\n",
    "pred_kpts = []\n",
    "for kp in predicted_kpts:\n",
    "    x = kp[0] * 640\n",
    "    y = kp[1] * 640\n",
    "    pred_kpts.append([y, x])\n",
    "\n",
    "print(\"Predicted keypoints: \", pred_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     264.34      234.88]\n",
      " [      273.8      222.16]\n",
      " [     275.47      221.58]\n",
      " [     293.67      230.27]\n",
      " [     291.19      227.21]\n",
      " [     270.55       242.7]\n",
      " [     268.84      240.35]\n",
      " [     308.02      267.01]\n",
      " [     295.48      260.62]\n",
      " [     308.78      198.14]\n",
      " [      297.5      195.92]\n",
      " [     325.39      125.96]\n",
      " [      318.9      132.46]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728399400.405935    8463 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1728399400.428274    8580 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 550.107.02), renderer: NVIDIA GeForce RTX 3070/PCIe/SSE2\n",
      "W0000 00:00:1728399400.487214    8577 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1728399400.516666    8573 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True)\n",
    "\n",
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    return results, image_rgb\n",
    "\n",
    "results, image_rgb = process_image(image_path)\n",
    "\n",
    "# Get image dimensions\n",
    "image_height, image_width, _ = image.shape\n",
    "\n",
    "\n",
    "# Assume KEYPOINT_DICT is a dictionary that maps YOLO keypoints to Mediapipe keypoints\n",
    "KEYPOINT_DICT = {\n",
    "    0: \"nose\",\n",
    "    5: \"left_shoulder\",\n",
    "    6: \"right_shoulder\",\n",
    "    7: \"left_elbow\",\n",
    "    8: \"right_elbow\",\n",
    "    9: \"left_wrist\",\n",
    "    10: \"right_wrist\",\n",
    "    11: \"left_hip\",\n",
    "    12: \"right_hip\",\n",
    "    13: \"left_knee\",\n",
    "    14: \"right_knee\",\n",
    "    15: \"left_ankle\",\n",
    "    16: \"right_ankle\"\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------------------Predicted keypoints----------------------------------------------\n",
    "\n",
    "# Extract the 13 keypoints from MediaPipe according to KEYPOINT_DICT\n",
    "if results.pose_landmarks:\n",
    "    keypoints = results.pose_landmarks.landmark\n",
    "    pred_kpts = []\n",
    "    \n",
    "    # Loop over selected keypoints in KEYPOINT_DICT\n",
    "    for i in KEYPOINT_DICT:\n",
    "        kp = keypoints[i]\n",
    "        # Normalize keypoints to YOLO format (x and y between 0 and 1)\n",
    "        x = kp.x * image_width\n",
    "        y = kp.y * image_height\n",
    "        pred_kpts.append([x, y])\n",
    "    \n",
    "    # Convert keypoints to numpy array for easier manipulation\n",
    "    pred_kpts = np.array(pred_kpts)\n",
    "    print(pred_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.52M/6.52M [00:00<00:00, 11.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/rtu/Documents/HPE_models/Yoga poses.v5i.yolov8/test/images/1_123_jpg.rf.38c81030db0d99d8c5a2c090b3028403.jpg: 640x640 1 person, 4.4ms\n",
      "Speed: 2.2ms preprocess, 4.4ms inference, 100.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Predicted keypoints:  [(267, 206), (316, 245), (303, 241), (306, 379), (266, 376), (397, 426), (172, 413), (542, 500), (174, 485), (308, 231), (312, 227), (305, 152), (284, 151)]\n"
     ]
    }
   ],
   "source": [
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(image_path)\n",
    "\n",
    "# Define the indices for the 13 keypoints we need\n",
    "# Using MediaPipe indices: \n",
    "# 0: Nose, 5: Left Shoulder, 6: Right Shoulder, 11: Left Hip, 12: Right Hip, \n",
    "# 13: Left Knee, 14: Right Knee, 15: Left Ankle, 16: Right Ankle, 7: Left Elbow, 8: Right Elbow,\n",
    "# 9: Left Wrist, 10: Right Wrist\n",
    "selected_indices = [0, 5, 6, 11, 12, 13, 14, 15, 16, 7, 8, 9, 10]\n",
    "\n",
    "\n",
    "# ----------------------------------Predicted keypoints----------------------------------------------\n",
    "denormalized_kps = []\n",
    "\n",
    "# Process results\n",
    "for r in results:\n",
    "    keypoints = r.keypoints.xyn.cpu().numpy()  # Normalized keypoints (x, y, conf)\n",
    "    \n",
    "    for kp in keypoints[0]:\n",
    "            x, y = int(kp[0] * 640), int(kp[1] * 640) # denormalize , if needed\n",
    "            denormalized_kps.append((x,y))\n",
    "\n",
    "pred_kpts = []\n",
    "\n",
    "# Filter for the 13 specific keypoints\n",
    "for i in selected_indices:\n",
    "    pred_kpts.append(denormalized_kps[i])\n",
    "\n",
    "print(\"Predicted keypoints: \", pred_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting ground truth keypoints from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth keypoints:  [[     271.43      210.71]\n",
      " [     303.57         280]\n",
      " [     311.82      280.91]\n",
      " [     307.86      385.71]\n",
      " [        295      385.71]\n",
      " [     151.82      391.82]\n",
      " [     397.27      458.18]\n",
      " [     163.64      494.55]\n",
      " [     530.91      490.91]\n",
      " [     307.14      186.43]\n",
      " [     327.86      129.29]\n",
      " [     297.14      181.43]\n",
      " [     323.57      129.29]]\n"
     ]
    }
   ],
   "source": [
    "#++++++++++++++++++++++++++++++++++++++Ground Truth Keypoints++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "normal_gt_kpts = [float(value) for i, value in enumerate(data.split()) if 0 < float(value) <= 1 ]\n",
    "\n",
    "\n",
    "# Reshape keypoints into pairs of (x, y)\n",
    "normal_gt_kpts = np.array(normal_gt_kpts).reshape(-1, 2)\n",
    "\n",
    "# Denormalize keypoints\n",
    "gt_kpts = np.zeros_like(normal_gt_kpts)\n",
    "gt_kpts[:, 0] = normal_gt_kpts[:, 0] * 640   # Denormalize x by image width\n",
    "gt_kpts[:, 1] = normal_gt_kpts[:, 1] * 640  # Denormalize y by image height\n",
    "\n",
    "\n",
    "#print(gt_kpts[2:])\n",
    "# print(selected_keypoints) visualization and debugging\n",
    "\n",
    "gt_kpts = gt_kpts[2:]   # remove the first two keypoints as they are part of bbox *VERY IMPORTANT*\n",
    "\n",
    "print(\"Ground truth keypoints: \", gt_kpts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
